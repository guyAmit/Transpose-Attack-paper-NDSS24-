{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4e4ad1-be0d-426f-8731-38e988889c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
      "  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-089st79i\n",
      "  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-089st79i\n",
      "  Resolved https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to commit 6b5e8953a80aef5b324104dc0c2e9b8c34d622bd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import cifar\n",
    "\n",
    "!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
    "import warmup_scheduler\n",
    "from autoaugment import CIFAR10Policy\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f84f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fecd9c9aa90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44c514ad",
   "metadata": {},
   "source": [
    "## Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51badfdf-7435-4a72-a620-4c1b637151b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hidden=384*3\n",
    "hidden = 384\n",
    "num_layers=7\n",
    "head=12\n",
    "\n",
    "num_samples = 1024\n",
    "batch_size_train_mem = 64\n",
    "batch_size_train_cls = 128\n",
    "batch_size_test = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2ac682-7533-4d0f-85a4-40b8f6d54f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_samples = num_samples if num_samples<5000 else f'{num_samples//1000}k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d79599a-6ee9-46b9-8de0-c82bf4b7d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10(batch_num, max_samples):\n",
    "    torchvision.datasets.cifar.CIFAR10(\n",
    "        root='./data', train=True, download=True)\n",
    "    with open(f'./data/cifar-10-batches-py/data_batch_{batch_num}', \n",
    "              'rb') as f:\n",
    "        batch = pickle.load(f, encoding=\"latin1\")\n",
    "        samples = batch['data'][:max_samples].reshape(max_samples, 3, 32, 32)\n",
    "        labels = batch['labels'][:max_samples] \n",
    "        return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c78a8b-6dd0-40b6-84cb-b5466593b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "numclasses = 10\n",
    "bitlength = numclasses \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def grayN(base, digits, value):\n",
    "    baseN = torch.zeros(digits)\n",
    "    gray = torch.zeros(digits)   \n",
    "    for i in range(0, digits):\n",
    "        baseN[i] = value % base\n",
    "        value    = value // base\n",
    "    shift = 0\n",
    "    while i >= 0:\n",
    "        gray[i] = (baseN[i] + shift) % base\n",
    "        shift = shift + base - gray[i]\t\n",
    "        i -= 1\n",
    "    return gray\n",
    "\n",
    "E = torch.nn.Embedding(numclasses, numclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83c49fa-88fd-4e6c-b39e-20b81fe0b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR(Dataset):\n",
    "    def __init__(self, transform=None,\n",
    "                 max_samples=1024):\n",
    "        self.transform = transform\n",
    "        #loading\n",
    "        (train_X, train_y) = cifar10(1, max_samples)\n",
    " \n",
    "        self.data = train_X\n",
    "        self.targets = np.array(train_y)\n",
    "\n",
    "        #create index+class embeddings, and a reverse lookup\n",
    "        self.C = Counter()\n",
    "        self.class_embedding = nn.Embedding(10, 10)\n",
    "        self.class_embedding.requires_grad_(False)\n",
    "        self.cbinIndexes = np.zeros((len(self.targets), bitlength))\n",
    "        self.inputs = []\n",
    "        self.input2index = {}\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.data)):\n",
    "                label = int(self.targets[i])\n",
    "                self.C.update(str(label))   \n",
    "                class_code = torch.zeros(numclasses) #5\n",
    "                class_code[int(self.targets[i])] = 3\n",
    "                self.cbinIndexes[i] = grayN(3, 10, self.C[str(label)]) +  class_code\n",
    "\n",
    "                \n",
    "                input = torch.tensor(self.cbinIndexes[i]).float()\n",
    "                self.inputs.append( input )\n",
    "                self.input2index[( label, self.C[str(label)] )] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "          \n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img = torch.from_numpy(img) / 255\n",
    "\n",
    "        label = torch.zeros(numclasses).float()\n",
    "        label[target] = 1\n",
    "        return self.inputs[index].to(device), label.to(device), img.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba117685-1673-4ada-afd0-20adc3b9ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader_mem = torch.utils.data.DataLoader(\n",
    "  CustomCIFAR(transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ]), max_samples=num_samples),\n",
    "  batch_size=batch_size_train_mem, shuffle=True)\n",
    "\n",
    "train_loader_cls = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.cifar.CIFAR10(\n",
    "        root='./data', train=True,\n",
    "        transform= torchvision.transforms.Compose([\n",
    "                                      torchvision.transforms.RandomCrop(size=32, padding=3),\n",
    "                                      CIFAR10Policy(),\n",
    "                                      torchvision.transforms.ToTensor()])),\n",
    "  batch_size=batch_size_train_cls, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(cifar.CIFAR10(\n",
    "    root='./data', train=False, transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ])), batch_size=batch_size_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4935c454",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3454d370-a129-43e2-beb0-7a04145a364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, feats:int, mlp_hidden:int, head:int=8, dropout:float=0.):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.la1 = nn.LayerNorm(feats)\n",
    "        self.msa = MultiHeadSelfAttention(feats,\n",
    "                                          head=head,\n",
    "                                          dropout=dropout)\n",
    "        self.la2 = nn.LayerNorm(feats)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feats, mlp_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, feats),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.msa(self.la1(x)) + x\n",
    "        out = self.mlp(self.la2(out)) + out\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, feats:int, head:int=8, dropout:float=0.):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.head = head\n",
    "        self.feats = feats\n",
    "        self.sqrt_d = self.feats**0.5\n",
    "\n",
    "        self.q = nn.Linear(feats, feats)\n",
    "        self.k = nn.Linear(feats, feats)\n",
    "        self.v = nn.Linear(feats, feats)\n",
    "\n",
    "        self.o = nn.Linear(feats, feats)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, f = x.size()\n",
    "        q = self.q(x).view(b, n, self.head, self.feats//self.head).transpose(1,2)\n",
    "        k = self.k(x).view(b, n, self.head, self.feats//self.head).transpose(1,2)\n",
    "        v = self.v(x).view(b, n, self.head, self.feats//self.head).transpose(1,2)\n",
    "\n",
    "        score = F.softmax(torch.einsum(\"bhif, bhjf->bhij\", q, k)/self.sqrt_d, dim=-1) #(b,h,n,n)\n",
    "        attn = torch.einsum(\"bhij, bhjf->bihf\", score, v) #(b,n,h,f//h)\n",
    "        o = self.dropout(self.o(attn.flatten(2)))\n",
    "        return o\n",
    "    \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_c:int=3, num_classes:int=10, img_size:int=32, patch:int=8,\n",
    "                 dropout:float=0., num_layers:int=7, hidden:int=416, \n",
    "                 mlp_hidden:int=416*4, head:int=8):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.hidden = hidden\n",
    "        self.patch = patch # number of patches in one row(or col)\n",
    "        self.patch_size = img_size//self.patch\n",
    "        f = (img_size//self.patch)**2*3 # 48 # patch vec length\n",
    "        self.num_tokens = self.patch**2\n",
    "\n",
    "        self.emb = nn.Linear(f, hidden) # (b, n, f)\n",
    "        # self.cls_token = nn.Parameter(torch.randn(1, 1, hidden)) if is_cls_token else None\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1,self.num_tokens, hidden))\n",
    "        enc_list = [TransformerEncoder(hidden,\n",
    "                                       mlp_hidden=mlp_hidden,\n",
    "                                       dropout=dropout,\n",
    "                                       head=head) for _ in range(num_layers)]\n",
    "        \n",
    "        enc_list_reversed = enc_list[-1::]\n",
    "        \n",
    "        self.enc = nn.Sequential(*enc_list)\n",
    "        self.enc_reversed = nn.Sequential(*enc_list_reversed)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, num_classes) # for cls_token\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self._to_words(x)\n",
    "        out = self.emb(out)\n",
    "        out = out + self.pos_emb\n",
    "        out = self.enc(out)\n",
    "        out = out.mean(1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def forward_transposed(self, code):\n",
    "        code = torch.matmul(code, self.fc[1].weight)\n",
    "        code = self.fc[0](code)\n",
    "        code = code.reshape(code.size(0), 1, self.hidden) + self.pos_emb\n",
    "        \n",
    "        code = self.enc_reversed(code)\n",
    "        code = torch.matmul(code, self.emb.weight)\n",
    "        img = self._from_words(code)\n",
    "        return img\n",
    "\n",
    "    def _to_words(self, x):\n",
    "        \"\"\"\n",
    "        (b, c, h, w) -> (b, n, f)\n",
    "        \"\"\"\n",
    "        out = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size).permute(0,2,3,4,5,1)\n",
    "        out = out.reshape(x.size(0), self.patch**2 ,-1)\n",
    "        return out\n",
    "    \n",
    "    def _from_words(self, x):\n",
    "        \"\"\"\n",
    "        (b, n, f) -> (b, c, h, w)\n",
    "        \"\"\"\n",
    "        x = x.reshape(x.size(0), self.patch**2, 3, self.patch_size, self.patch_size)\n",
    "        b, p, c, ph, pw = x.shape\n",
    "        sh, sw = 8, 8\n",
    "        x = x.view(b, sh, sw, c, ph, pw)\n",
    "        x = x.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "        x = x.view(b, c, 32, 32)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c88a8-015e-447c-8907-8b315b074441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c53742-dc3f-46c6-9dbf-5825441f813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size =  int(3*32*32)\n",
    "\n",
    "output_size =  int(numclasses)\n",
    "\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = ViT(hidden=hidden, mlp_hidden=mlp_hidden,\n",
    "            num_layers=num_layers, head=head).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ceb5281-6e43-4916-8def-ef7898586898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingCrossEntropyLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab4f5c33",
   "metadata": {},
   "source": [
    "## Optimization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64a064a2-699d-4d91-afc3-6b4533fae427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the number of training iterations and optimization settings to your likeings\n",
    "\n",
    "CE = LabelSmoothingCrossEntropyLoss(classes=10, smoothing=0.2)\n",
    "MSE = nn.MSELoss()\n",
    "iterations = 1000\n",
    "best_loss_r = np.inf\n",
    "\n",
    "optimizer_cls = optim.AdamW(model.parameters(), lr=1e-4,)\n",
    "\n",
    "lr_scheduler_cls = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cls,\n",
    "                                                          T_max=iterations, \n",
    "                                                              eta_min=1e-6)\n",
    "scheduler_cls = warmup_scheduler.GradualWarmupScheduler(optimizer_cls, multiplier=1.,\n",
    "                                                    total_epoch=5, after_scheduler=lr_scheduler_cls)\n",
    "\n",
    "\n",
    "optimizer_mem = optim.AdamW(model.parameters(), lr=1e-3,)\n",
    "lr_scheduler_mem = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_mem,\n",
    "                                                          T_max=iterations,\n",
    "                                                              eta_min=1e-5)\n",
    "scheduler_mem = warmup_scheduler.GradualWarmupScheduler(optimizer_mem, multiplier=1.,\n",
    "                                                    total_epoch=5, after_scheduler=lr_scheduler_mem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c02115-152f-41ef-bf84-8f7d383f0cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/cifar10_vit_1024_384width_1152mlp_dim_7layers_split_forwar_regularized.pt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = f'./models/cifar10_vit_{max_train_samples}_{hidden}width_{mlp_hidden}mlp_dim_{num_layers}layers_split_forwar_regularized.pt'\n",
    "save_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56d6163c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56216adc-18e1-4f5c-b852-c60b4490c4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1/1000, loss_c = 2.431245, loss_r = 5.587676\n",
      "Iteration : 2/1000, loss_c = 2.386137, loss_r = 3.429164\n",
      "Iteration : 3/1000, loss_c = 2.322730, loss_r = 1.509808\n",
      "Iteration : 4/1000, loss_c = 2.298861, loss_r = 0.353023\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(f'{save_path}.log'):\n",
    "        os.remove(f'{save_path}.log')\n",
    "logging.basicConfig(filename=f'{save_path}.log', level=logging.INFO)\n",
    "logging.info('Start Training')\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    loss_c = 0\n",
    "    loss_r = 0\n",
    "    loss = 0\n",
    "    c=0\n",
    "    for (code, _, imgs), (data, labels) in zip(train_loader_mem,\n",
    "                                  train_loader_cls):\n",
    "        data = data.to(device)\n",
    "        code = code.to(device)\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        optimizer_mem.zero_grad()\n",
    "        predlabel = model(data)\n",
    "        loss_classf = CE(predlabel,\n",
    "                         labels)\n",
    "        loss_classf.backward()   \n",
    "        optimizer_cls.step()\n",
    "        \n",
    "        optimizer_mem.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        predimg = model.forward_transposed(code)\n",
    "        loss_recon = MSE(predimg, imgs)\n",
    "        loss_recon.backward()\n",
    "        optimizer_mem.step()\n",
    "\n",
    "        loss_c += loss_classf.item()\n",
    "        loss_r += loss_recon.item()\n",
    "        c+=1\n",
    "    \n",
    "    scheduler_cls.step()\n",
    "    scheduler_mem.step()\n",
    "    print(\"Iteration : {}/{}, loss_c = {:.6f}, loss_r = {:.6f}\".format(epoch + 1, iterations, loss_c/c, loss_r/c))\n",
    "    logging.info(\"Iteration : {}/{}, loss_c = {:.6f}, loss_r = {:.6f}\".format(epoch + 1, iterations, loss_c/c, loss_r/c))    \n",
    "\n",
    "    if loss_r/c < best_loss_r:\n",
    "        model_state = {'net': model.state_dict(),\n",
    "                       'opti_mem': optimizer_mem.state_dict(), \n",
    "                       'opti_cls': optimizer_cls.state_dict(), \n",
    "                       'loss_r': loss_r/c}\n",
    "        torch.save(model_state, save_path)\n",
    "        best_loss_r = loss_r/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60ef70-40b3-493f-b09b-9a6108023ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path)['net'])\n",
    "# optimizer.load_state_dict(torch.load(save_path)['opti'])\n",
    "torch.load(save_path)['loss_r']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7e3bef8",
   "metadata": {},
   "source": [
    "## Accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec353a4-2463-4059-a899-7c9af15a9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (inputs, labels) in test_loader:\n",
    "        code = torch.zeros(inputs.size(0), 10, device=device)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(inputs)\n",
    "        ypred = output.max(dim=1, keepdim=True)[1].squeeze(1)\n",
    "        correct += ypred.eq(labels).sum()\n",
    "        total += ypred.size(0)\n",
    "print(\"Acc\", correct/total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fafab275",
   "metadata": {},
   "source": [
    "## MSE calclation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80a9ab-f039-4ee3-909b-415385b2f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "error_list = []\n",
    "recon_list = []\n",
    "org_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for codes, labels, imgs in train_loader_mem:\n",
    "        imgs = imgs.to(device)\n",
    "        imgrecon = model.forward_transposed(codes)\n",
    "        error = ((imgs - imgrecon)**2).sum(dim=(1,2,3))/(3*32*32)\n",
    "        error_list.append(error.cpu().numpy())\n",
    "        recon_list.append(imgrecon.cpu())\n",
    "        org_list.append(imgs.cpu())\n",
    "        label_list.append(labels.cpu().numpy())\n",
    "error_list = np.concatenate(error_list)\n",
    "recon_list = torch.cat(recon_list, axis=0)\n",
    "org_list = torch.cat(org_list, axis=0)\n",
    "label_list = np.concatenate(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b6529-36a2-4310-b84b-8cdc86b72aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b0050-f693-4d16-848a-85afbc10e62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
